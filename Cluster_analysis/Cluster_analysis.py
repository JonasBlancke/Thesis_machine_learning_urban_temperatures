# -*- coding: utf-8 -*-
"""Kopie van results_to-raster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1byt5xMUmVo4g-xb_3VNrDpmjUA_ma6a3
"""

!pip install netCDF4
!pip install -q sklearn
! pip install git+https://github.com/pydata/xarray.git
!pip install xarray
#!pip install --user cdsapi # for Python 2.7
!pip install rasterio
!pip install rioxarray
#%pip install mlxtend --upgrade
#%tensorflow_version 2.x  # this line is not required unless you are in a notebook

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import tensorflow as tf
import xarray as xr
import rasterio
import rioxarray as rxr
from osgeo import gdal
from osgeo import osr
from osgeo import ogr

from google.colab import drive
drive.mount('/content/drive')

"""# **CLUSTER ANALYSIS**"""

pip install matplotlib seaborn

# Specify the file path
#excel_file_path = '/content/drive/MyDrive/thesis/cluster/FINAL_2.xlsx'
excel_file_path = '/content/drive/MyDrive/thesis/cluster/CLUSTER_28DEC_new2.xlsx'




# Read the Excel file into a pandas DataFrame
df = pd.read_excel(excel_file_path)
df=df.rename(columns={'latitude':'Latitude',  'longitude': 'Longitude'  ,'rangschikking':'KÃ¶ppen_class','E_WR_T_14': "Temperature", 'E_WR_P_14': "Precipitation", 'Average_Population_Band_Value': "Population density", 'Average_Elevation_Band_Value': "Elevation",'Average_Coast_Band_Value': "Distance coast", "Average_height_Band_Value": "Building height", 'Average_Surface_Band_Value': "The built-up fraction", "Average_LCZ_water": "LCZ_water",  "Average_LCZ_built": "LCZ_built", "Average_LCZ_tree": "LCZ_tree", "Average_LCZ_bareandveg": "LCZ_bare_soil_and_low_veg", 'Average_Imperv_Band_Value':'Imperviousness density', 'Average_AHF_Band_Value': 'AHF', 'AREA': 'Urban centre area'})
df.columns
df['Dist_coast_log'] = np.log10(df['Distance coast'])

df['LCZ_vegetation']=df['LCZ_bare_soil_and_low_veg']+df['LCZ_tree']
#data=df[['AHF', 'E_WR_P_14', 'E_WR_T_14', 'EL_AV_ALS', 'Min_coast', 'Height', 'Size', 'green' ]]
#data=df[["Mean_temperature", "Mean_precipitation",  "Average_elevation","Min_dist_coast","Mean_building_height",  'AHF', 'Size',   "Mean_built-up_surface", "LCZ_vegetation", 'Population','latitude', 'longitude' ]]
#data=df[['E_WR_T_14', 'EL_AV_ALS','Min_coast',  'green', 'Population', 'Size','E_WR_P_14']]
data=df[['Elevation', 'Longitude', 'Latitude', 'Precipitation', 'Temperature', 'Dist_coast_log', 'AHF']]

"""# **on non transformed**"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import scale
from sklearn.utils import resample
from sklearn.cluster import KMeans

X = data
# Apply Min-Max scaling to the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split
np.random.seed(44)

kmeans = KMeans(n_clusters=3)

#kmeans.fit(X_scaled)  #how many PCA's
#data['Cluster'] = kmeans.predict(X_scaled) #How many

#data['Cluster'] = kmeans.predict(X_scaled) #How many
kmeans.fit(X_scaled)  #how many PCA's
data['Cluster'] = kmeans.predict(X_scaled) #How many

clustered = pd.concat([data['Cluster'], df[['Latitude', 'Longitude']]], axis=1)
cluster_results=pd.concat([data['Cluster'], df['City']], axis=1)
cluster_results['Cluster'].unique()

import folium
data=clustered
# Create a map centered around a specific location (e.g., the mean of the latitudes and longitudes)
map_center = [data['Latitude'].mean(), data['Longitude'].mean()]
mymap = folium.Map(location=map_center, zoom_start=5)

# Iterate through the rows and add markers to the map for each data point
for row in data.itertuples():
    lat, lon, cluster = row.Latitude, row.Longitude, row.Cluster
    #color = ['blue', 'green', 'red', 'purple', 'orange'][cluster]  # Assign a color based on the cluster
    #color = ['blue', 'green', 'red', 'purple','black', 'yellow'][cluster]
    color = ['blue', 'green', 'red'][cluster]
    folium.CircleMarker(location=[lat, lon], radius=5, color=color, fill=True, fill_color=color, fill_opacity=0.7).add_to(mymap)

# Display the map
mymap.save("/content/drive/MyDrive/thesis/cluster/non_transformed_clustered_map.html")


observation_counts = data['Cluster'].value_counts()
print(observation_counts)

"""# **on PCA trasofrmed**"""

import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
# Create a subset of the dataset containing only the selected features
data=df[['Elevation', 'Longitude', 'Latitude', 'Precipitation', 'Temperature', 'Dist_coast_log', 'AHF']]

subset_data = data
np.random.seed(44)

# Select the relevant columns for clustering
X = subset_data
# Apply Min-Max scaling to the data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
# Calculate the correlation matrix
correlation_matrix = subset_data.corr()

# Create a heatmap to visualize the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation matrix of clustering features', fontsize=16)
plt.show()

from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import scale
from sklearn.utils import resample

import numpy as np
import pandas as pd
def return_noc_pca(var, threshold):
    sumvar = 0.
    i = 0
    while sumvar < threshold:
        sumvar+=var[i]
        i+=1
    return i


def return_noc_pca_fast(var, threshold):
    return np.where(np.cumsum(var)>threshold)[0][0]

pca = PCA()
pca.fit(X_scaled)
X_transformed = pca.transform(X_scaled)
var = pca.explained_variance_ratio_
print(var)
print(np.cumsum(var))

fig, ax = plt.subplots()
ax.bar(np.arange(1,var.shape[0]+1),var,color='g');
ax.set_xlabel('Principal Component', size=18)
ax.set_ylabel('Explained variance', size=18)
plt.show()

# Assuming cumvar is calculated before this point
cumvar = np.cumsum(var)

# Plotting the cumulative explained variance with a line plot
fig, ax = plt.subplots()
ax.plot(np.arange(1, cumvar.shape[0] + 1), cumvar, color='g', marker='o', linestyle='-')

# Find the index where the cumulative explained variance crosses 0.8 using interpolation
intersection_index = np.interp(0.8, cumvar, np.arange(1, cumvar.shape[0] + 1))
# Add a black dotted line at y=0.8
ax.axhline(y=0.8, color='black', linestyle='--', label='variance threshold', xmin=0, xmax=0.42)


# Add a vertical line from the intersection point down to the x-axis
ax.axvline(x=intersection_index, color='black', linestyle='--',  ymin=0, ymax=0.680)

ax.set_xlabel('Principal Component', size=18)
ax.set_ylabel('Cumulative Explained Variance', size=18)  # Show legend
ax.legend()  # Show legend

plt.show()

from sklearn.cluster import KMeans
##2:

np.random.seed(44)


possible_k = np.arange(2,15)
si = np.zeros(len(possible_k))
i=0
for k in possible_k:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_scaled)
    si[i] = silhouette_score(X_scaled, kmeans.labels_)
    i+=1


si_pca_4 = np.zeros(len(possible_k))
si_pca_5 = np.zeros(len(possible_k))
si_pca_6 = np.zeros(len(possible_k))
si_pca_7 = np.zeros(len(possible_k))
si_pca_8 = np.zeros(len(possible_k))
si_pca_9 = np.zeros(len(possible_k))

i=0
for k in possible_k:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_transformed[:,0:4])
    si_pca_4[i] = silhouette_score(X_transformed[:,0:4], kmeans.labels_)
    #si_pca_5[i] = silhouette_score(X_transformed[:,0:5], kmeans.labels_)
    #si_pca_6[i] = silhouette_score(X_transformed[:,0:6], kmeans.labels_)
    #si_pca_7[i] = silhouette_score(X_transformed[:,0:7], kmeans.labels_)
    #si_pca_8[i] = silhouette_score(X_transformed[:,0:8], kmeans.labels_)
    #si_pca_9[i] = silhouette_score(X_transformed[:,0:3], kmeans.labels_)
    i+=1

fig, ax = plt.subplots()
ax.scatter(possible_k, si, label='Raw data');
ax.scatter(possible_k, si_pca_4, label='PCA transformed');
#ax.scatter(possible_k, si_pca_5, label='PCA transformed_5');
#ax.scatter(possible_k, si_pca_6, label='PCA transformed_6');
#ax.scatter(possible_k, si_pca_7, label='PCA transformed_7');
#ax.scatter(possible_k, si_pca_8, label='PCA transformed_8');
#ax.scatter(possible_k, si_pca_9, label='PCA transformed_9');
ax.set_xlabel('Number of clusters', size=18)
ax.set_ylabel('Silhouette-index', size=18)
plt.title('Silhouette-index: Raw data vs PCA with 4 principal components')
plt.legend()
plt.show()

from matplotlib.colors import ListedColormap

np.random.seed(44)
# KMeans clustering
kmeans = KMeans(n_clusters=3)
kmeans.fit(X_transformed[:, 0:4])

# Define custom colors for each cluster
cluster_colors = ['blue', 'green', 'red']

# Create a colormap with custom colors
cmap_custom = ListedColormap(cluster_colors)

# Plotting
fig, ax = plt.subplots()
scatter = ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=kmeans.labels_, cmap=cmap_custom)

ax.set_xlabel(r'$x_0$ (' + str(np.round(var[0], 3) * 100) + '%)', size=18)
ax.set_ylabel(r'$x_1$ (' + str(np.round(var[1], 3) * 100) + '%)', size=18)

# Create a legend with custom colors
legend_labels = [f'Cluster {i+1}' for i in range(len(cluster_colors))]
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in cluster_colors]
ax.legend(legend_elements, legend_labels)

plt.show()

from sklearn.model_selection import train_test_split
np.random.seed(44)

kmeans = KMeans(n_clusters=3)

#kmeans.fit(X_scaled)  #how many PCA's
#data['Cluster'] = kmeans.predict(X_scaled) #How many

#data['Cluster'] = kmeans.predict(X_scaled) #How many
kmeans.fit(X_transformed[:,0:4])  #how many PCA's
data['Cluster'] = kmeans.predict(X_transformed[:,0:4]) #How many

clustered = pd.concat([data['Cluster'], df[['Latitude', 'Longitude']]], axis=1)
cluster_results=pd.concat([data['Cluster'], df['City']], axis=1)
cluster_results['Cluster'].unique()
cluster_results.to_csv("/content/drive/MyDrive/thesis/cluster/results10.csv", index=False)

#train_set, test_set = train_test_split(cluster_results, test_size=0.3, stratify=cluster_results['Cluster'], random_state=42)